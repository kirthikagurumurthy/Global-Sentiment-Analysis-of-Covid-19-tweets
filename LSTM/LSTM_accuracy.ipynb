{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kirthika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kirthika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "#import tensorflow\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Date</th>\n",
       "      <th>country</th>\n",
       "      <th>compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>These chinese people on campus are so consider...</td>\n",
       "      <td>2020-01-28 23:41:13+00:00</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.5777</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>So the first month of the new decade went some...</td>\n",
       "      <td>2020-01-30 23:06:34+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>Coronavirus sounds nice and all but I'm actual...</td>\n",
       "      <td>2020-01-30 22:09:54+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>Also a good compendium of information on #nCoV...</td>\n",
       "      <td>2020-01-22 23:19:57+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>OMG I WAS JUST\\r\\n\\r\\nearlier today I was expl...</td>\n",
       "      <td>2020-01-21 23:45:18+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.9042</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168624</th>\n",
       "      <td>36014</td>\n",
       "      <td>One, so you're saying people only have the rig...</td>\n",
       "      <td>2020-06-19 19:58:00+00:00</td>\n",
       "      <td>united states</td>\n",
       "      <td>0.4490</td>\n",
       "      <td>Positive</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168625</th>\n",
       "      <td>36015</td>\n",
       "      <td>Kolkata Private hospitals face shortage of bed...</td>\n",
       "      <td>2020-06-19 19:58:00+00:00</td>\n",
       "      <td>india</td>\n",
       "      <td>-0.1372</td>\n",
       "      <td>Negative</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168626</th>\n",
       "      <td>36016</td>\n",
       "      <td>Reminder: Drive-thru testing is offered at the...</td>\n",
       "      <td>2020-06-19 19:58:00+00:00</td>\n",
       "      <td>united states</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>Positive</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168627</th>\n",
       "      <td>36017</td>\n",
       "      <td>All Trump supporters should be excited to atte...</td>\n",
       "      <td>2020-06-19 19:57:58+00:00</td>\n",
       "      <td>united states</td>\n",
       "      <td>0.6486</td>\n",
       "      <td>Positive</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168628</th>\n",
       "      <td>36018</td>\n",
       "      <td>The Illinois Department of Public Health annou...</td>\n",
       "      <td>2020-06-19 19:57:56+00:00</td>\n",
       "      <td>united states</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168629 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                             Tweets  \\\n",
       "0                1  These chinese people on campus are so consider...   \n",
       "1               19  So the first month of the new decade went some...   \n",
       "2               20  Coronavirus sounds nice and all but I'm actual...   \n",
       "3               25  Also a good compendium of information on #nCoV...   \n",
       "4               26  OMG I WAS JUST\\r\\n\\r\\nearlier today I was expl...   \n",
       "...            ...                                                ...   \n",
       "168624       36014  One, so you're saying people only have the rig...   \n",
       "168625       36015  Kolkata Private hospitals face shortage of bed...   \n",
       "168626       36016  Reminder: Drive-thru testing is offered at the...   \n",
       "168627       36017  All Trump supporters should be excited to atte...   \n",
       "168628       36018  The Illinois Department of Public Health annou...   \n",
       "\n",
       "                             Date        country  compound Sentiment  month  \n",
       "0       2020-01-28 23:41:13+00:00    afghanistan    0.5777  positive      1  \n",
       "1       2020-01-30 23:06:34+00:00        albania    0.2732  positive      1  \n",
       "2       2020-01-30 22:09:54+00:00        albania    0.2263  positive      1  \n",
       "3       2020-01-22 23:19:57+00:00        albania    0.4404  positive      1  \n",
       "4       2020-01-21 23:45:18+00:00        albania    0.9042  positive      1  \n",
       "...                           ...            ...       ...       ...    ...  \n",
       "168624  2020-06-19 19:58:00+00:00  united states    0.4490  Positive      6  \n",
       "168625  2020-06-19 19:58:00+00:00          india   -0.1372  Negative      6  \n",
       "168626  2020-06-19 19:58:00+00:00  united states    0.5106  Positive      6  \n",
       "168627  2020-06-19 19:57:58+00:00  united states    0.6486  Positive      6  \n",
       "168628  2020-06-19 19:57:56+00:00  united states    0.0000   Neutral      6  \n",
       "\n",
       "[168629 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('final_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Date</th>\n",
       "      <th>country</th>\n",
       "      <th>compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These chinese people on campus are so consider...</td>\n",
       "      <td>2020-01-28 23:41:13+00:00</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.5777</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So the first month of the new decade went some...</td>\n",
       "      <td>2020-01-30 23:06:34+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus sounds nice and all but I'm actual...</td>\n",
       "      <td>2020-01-30 22:09:54+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Also a good compendium of information on #nCoV...</td>\n",
       "      <td>2020-01-22 23:19:57+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMG I WAS JUST\\r\\n\\r\\nearlier today I was expl...</td>\n",
       "      <td>2020-01-21 23:45:18+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.9042</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  \\\n",
       "0  These chinese people on campus are so consider...   \n",
       "1  So the first month of the new decade went some...   \n",
       "2  Coronavirus sounds nice and all but I'm actual...   \n",
       "3  Also a good compendium of information on #nCoV...   \n",
       "4  OMG I WAS JUST\\r\\n\\r\\nearlier today I was expl...   \n",
       "\n",
       "                        Date      country  compound Sentiment  month  \n",
       "0  2020-01-28 23:41:13+00:00  afghanistan    0.5777  positive      1  \n",
       "1  2020-01-30 23:06:34+00:00      albania    0.2732  positive      1  \n",
       "2  2020-01-30 22:09:54+00:00      albania    0.2263  positive      1  \n",
       "3  2020-01-22 23:19:57+00:00      albania    0.4404  positive      1  \n",
       "4  2020-01-21 23:45:18+00:00      albania    0.9042  positive      1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lower(text):\n",
    "    #text = text.to_str()\n",
    "    text = str.lower(text)\n",
    "    return text\n",
    "df['Sentiment'] = df['Sentiment'].apply(lambda x: lower(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweets']=df['Tweets'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [word for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "df['Text_cleaned'] = df['Tweets'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Date</th>\n",
       "      <th>country</th>\n",
       "      <th>compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>month</th>\n",
       "      <th>Text_cleaned</th>\n",
       "      <th>Text_stemmed</th>\n",
       "      <th>Text_lemmatized_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These chinese people on campus are so consider...</td>\n",
       "      <td>2020-01-28 23:41:13+00:00</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.5777</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[chinese, people, campus, considerate, wear, m...</td>\n",
       "      <td>[chines, peopl, campu, consider, wear, mask, o...</td>\n",
       "      <td>[chine, peopl, campu, consider, wear, mask, ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So the first month of the new decade went some...</td>\n",
       "      <td>2020-01-30 23:06:34+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[first, month, new, decade, went, something, l...</td>\n",
       "      <td>[first, month, new, decad, went, someth, like,...</td>\n",
       "      <td>[first, month, new, decad, went, someth, like,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus sounds nice and all but I'm actual...</td>\n",
       "      <td>2020-01-30 22:09:54+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[coronavirus, sounds, nice, im, actually, hold...</td>\n",
       "      <td>[coronaviru, sound, nice, im, actual, hold, do...</td>\n",
       "      <td>[coronaviru, sound, nice, im, actual, hold, do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Also a good compendium of information on #nCoV...</td>\n",
       "      <td>2020-01-22 23:19:57+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[also, good, compendium, information, ncov, co...</td>\n",
       "      <td>[also, good, compendium, inform, ncov, coronav...</td>\n",
       "      <td>[also, good, compendium, inform, ncov, coronav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMG I WAS JUST\\r\\n\\r\\nearlier today I was expl...</td>\n",
       "      <td>2020-01-21 23:45:18+00:00</td>\n",
       "      <td>albania</td>\n",
       "      <td>0.9042</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[omg, earlier, today, explaining, coronavirus,...</td>\n",
       "      <td>[omg, earlier, today, explain, coronaviru, sit...</td>\n",
       "      <td>[omg, earlier, today, explain, coronaviru, sit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  \\\n",
       "0  These chinese people on campus are so consider...   \n",
       "1  So the first month of the new decade went some...   \n",
       "2  Coronavirus sounds nice and all but I'm actual...   \n",
       "3  Also a good compendium of information on #nCoV...   \n",
       "4  OMG I WAS JUST\\r\\n\\r\\nearlier today I was expl...   \n",
       "\n",
       "                        Date      country  compound Sentiment  month  \\\n",
       "0  2020-01-28 23:41:13+00:00  afghanistan    0.5777  positive      1   \n",
       "1  2020-01-30 23:06:34+00:00      albania    0.2732  positive      1   \n",
       "2  2020-01-30 22:09:54+00:00      albania    0.2263  positive      1   \n",
       "3  2020-01-22 23:19:57+00:00      albania    0.4404  positive      1   \n",
       "4  2020-01-21 23:45:18+00:00      albania    0.9042  positive      1   \n",
       "\n",
       "                                        Text_cleaned  \\\n",
       "0  [chinese, people, campus, considerate, wear, m...   \n",
       "1  [first, month, new, decade, went, something, l...   \n",
       "2  [coronavirus, sounds, nice, im, actually, hold...   \n",
       "3  [also, good, compendium, information, ncov, co...   \n",
       "4  [omg, earlier, today, explaining, coronavirus,...   \n",
       "\n",
       "                                        Text_stemmed  \\\n",
       "0  [chines, peopl, campu, consider, wear, mask, o...   \n",
       "1  [first, month, new, decad, went, someth, like,...   \n",
       "2  [coronaviru, sound, nice, im, actual, hold, do...   \n",
       "3  [also, good, compendium, inform, ncov, coronav...   \n",
       "4  [omg, earlier, today, explain, coronaviru, sit...   \n",
       "\n",
       "                             Text_lemmatized_stemmed  \n",
       "0  [chine, peopl, campu, consider, wear, mask, ot...  \n",
       "1  [first, month, new, decad, went, someth, like,...  \n",
       "2  [coronaviru, sound, nice, im, actual, hold, do...  \n",
       "3  [also, good, compendium, inform, ncov, coronav...  \n",
       "4  [omg, earlier, today, explain, coronaviru, sit...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df['Text_stemmed'] = df['Text_cleaned'].apply(lambda x: stemming(x))\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "df['Text_lemmatized_stemmed'] = df['Text_stemmed'].apply(lambda x: lemmatizer(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "le = LabelEncoder() \n",
    "#df['deceptive']= le.fit_transform(df['deceptive'])\n",
    "df['polarity']= le.fit_transform(df['Sentiment'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97080     [dear, houthi, abduct, entir, countri, mr, kha...\n",
       "82276     [usatoday, network, tweet, fact, updat, pandem...\n",
       "88308     [dyk, peopl, fulli, recov, covid, antibodi, pl...\n",
       "119009    [fantasi, love, thiswalesnatureweek, ukgovern,...\n",
       "80584     [impact, coronaviru, shutdown, mean, mani, bus...\n",
       "                                ...                        \n",
       "6325      [interest, public, health, infecti, diseas, su...\n",
       "91310     [logic, question, covid, natur, viru, crazi, c...\n",
       "91322     [step, daughter, old, bae, scold, slack, schoo...\n",
       "120554    [princ, joachim, belgium, test, posit, coronav...\n",
       "20153     [china, coronaviru, outbreak, first, case, rep...\n",
       "Name: Text_lemmatized_stemmed, Length: 134903, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(df['polarity']).values\n",
    "X = df['Text_lemmatized_stemmed']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "TOP_K = 30000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val = X_train.values, X_test.values\n",
    "x_train, x_val, word_index, max_length = sequence_vectorize(x_train, x_val)\n",
    "num_features = min(len(word_index) + 1, TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 134903 samples, validate on 33726 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/20\n",
      "134903/134903 [==============================] - 112s 831us/sample - loss: 0.5860 - acc: 0.7725 - val_loss: 0.4259 - val_acc: 0.8527\n",
      "Epoch 2/20\n",
      "134903/134903 [==============================] - 115s 854us/sample - loss: 0.4214 - acc: 0.8552 - val_loss: 0.4055 - val_acc: 0.8566\n",
      "Epoch 3/20\n",
      "134903/134903 [==============================] - 114s 843us/sample - loss: 0.3769 - acc: 0.8712 - val_loss: 0.4122 - val_acc: 0.8560\n",
      "Epoch 4/20\n",
      "134903/134903 [==============================] - 112s 833us/sample - loss: 0.3430 - acc: 0.8818 - val_loss: 0.4115 - val_acc: 0.8565\n",
      "Epoch 5/20\n",
      "134903/134903 [==============================] - 111s 825us/sample - loss: 0.3200 - acc: 0.8899 - val_loss: 0.4181 - val_acc: 0.8563\n",
      "Epoch 6/20\n",
      "134903/134903 [==============================] - 111s 825us/sample - loss: 0.2990 - acc: 0.8969 - val_loss: 0.4335 - val_acc: 0.8550\n",
      "Epoch 7/20\n",
      "134903/134903 [==============================] - 112s 827us/sample - loss: 0.2838 - acc: 0.9018 - val_loss: 0.4459 - val_acc: 0.8542\n",
      "Epoch 8/20\n",
      "134903/134903 [==============================] - 112s 830us/sample - loss: 0.2710 - acc: 0.9075 - val_loss: 0.4662 - val_acc: 0.8530\n",
      "Epoch 9/20\n",
      "134903/134903 [==============================] - 112s 829us/sample - loss: 0.2629 - acc: 0.9096 - val_loss: 0.4808 - val_acc: 0.8530\n",
      "Epoch 10/20\n",
      "134903/134903 [==============================] - 112s 829us/sample - loss: 0.2513 - acc: 0.9131 - val_loss: 0.4776 - val_acc: 0.8518\n",
      "Epoch 11/20\n",
      "134903/134903 [==============================] - 112s 829us/sample - loss: 0.2454 - acc: 0.9163 - val_loss: 0.4905 - val_acc: 0.8509\n",
      "Epoch 12/20\n",
      "134903/134903 [==============================] - 111s 826us/sample - loss: 0.2364 - acc: 0.9186 - val_loss: 0.5098 - val_acc: 0.8510\n",
      "Epoch 13/20\n",
      "134903/134903 [==============================] - 111s 826us/sample - loss: 0.2314 - acc: 0.9213 - val_loss: 0.5148 - val_acc: 0.8486\n",
      "Epoch 14/20\n",
      "134903/134903 [==============================] - 112s 830us/sample - loss: 0.2264 - acc: 0.9233 - val_loss: 0.5124 - val_acc: 0.8498\n",
      "Epoch 15/20\n",
      "134903/134903 [==============================] - 112s 828us/sample - loss: 0.2225 - acc: 0.9241 - val_loss: 0.5202 - val_acc: 0.8474\n",
      "Epoch 16/20\n",
      "134903/134903 [==============================] - 112s 829us/sample - loss: 0.2166 - acc: 0.9263 - val_loss: 0.5211 - val_acc: 0.8493\n",
      "Epoch 17/20\n",
      "134903/134903 [==============================] - 111s 824us/sample - loss: 0.2122 - acc: 0.9274 - val_loss: 0.5430 - val_acc: 0.8483\n",
      "Epoch 18/20\n",
      "134903/134903 [==============================] - 109s 811us/sample - loss: 0.2078 - acc: 0.9296 - val_loss: 0.5545 - val_acc: 0.8465\n",
      "Epoch 19/20\n",
      "134903/134903 [==============================] - 109s 811us/sample - loss: 0.2044 - acc: 0.9303 - val_loss: 0.5691 - val_acc: 0.8447\n",
      "Epoch 20/20\n",
      "134903/134903 [==============================] - 112s 833us/sample - loss: 0.1994 - acc: 0.9315 - val_loss: 0.5739 - val_acc: 0.8449\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "embedding_vector_length = 256\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(TOP_K, embedding_vector_length, input_length=max_length),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist1 = model.fit(x_train, y_train, validation_data=(x_val, y_test), epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33726/33726 [==============================] - 3s 102us/sample - loss: 0.5739 - acc: 0.8449\n",
      "Test set\n",
      "  Loss: 0.574\n",
      "  Accuracy: 0.845\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(x_val,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4553674b3049>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lstm.sav'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "filename = 'lstm.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Date</th>\n",
       "      <th>country</th>\n",
       "      <th>compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>month</th>\n",
       "      <th>Text_cleaned</th>\n",
       "      <th>Text_stemmed</th>\n",
       "      <th>Text_lemmatized_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Tweets, Date, country, compound, Sentiment, month, Text_cleaned, Text_stemmed, Text_lemmatized_stemmed]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Sentiment\"] == \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
