{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kirthika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kirthika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "#import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "      <th>Vader-sentiment value</th>\n",
       "      <th>Vader-sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I wish quick recovery for talented @TOLOnews J...</td>\n",
       "      <td>2020-04-29 23:33:40+00:00</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>These chinese people on campus are so consider...</td>\n",
       "      <td>2020-01-28 23:41:13</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.5777</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>China‚Äôs Leader, Under Fire, Says He Led Corona...</td>\n",
       "      <td>2020-02-15 23:17:09</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>-0.6124</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>RT @DarrenEuronews: Top 5 countries üåç with hig...</td>\n",
       "      <td>Sat Jun 13 07:50:54 +0000 2020</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>RT @MileyCyrus: Spain, you united in solidarit...</td>\n",
       "      <td>Sat Jun 13 09:52:29 +0000 2020</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113566</th>\n",
       "      <td>30500</td>\n",
       "      <td>30500</td>\n",
       "      <td>RT @YourAnonNews: Yesterday, there were 140,92...</td>\n",
       "      <td>Sat Jun 13 11:24:47 +0000 2020</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113567</th>\n",
       "      <td>30501</td>\n",
       "      <td>30501</td>\n",
       "      <td>RT @iloharare: In the face of #covid19, we run...</td>\n",
       "      <td>Sat Jun 13 14:12:11 +0000 2020</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>-0.3274</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113568</th>\n",
       "      <td>30502</td>\n",
       "      <td>30502</td>\n",
       "      <td>RT @SAfridiOfficial: I‚Äôve been feeling unwell ...</td>\n",
       "      <td>Sat Jun 13 09:52:26 +0000 2020</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>-0.8020</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113569</th>\n",
       "      <td>30503</td>\n",
       "      <td>30503</td>\n",
       "      <td>It must be this Gvt which is worse than corona...</td>\n",
       "      <td>2020-03-02 23:49:25</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113570</th>\n",
       "      <td>30504</td>\n",
       "      <td>30504</td>\n",
       "      <td>BREAKING: Deputy leader of Iran's parliament s...</td>\n",
       "      <td>2020-03-03 23:57:40</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113571 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Unnamed: 0.1  \\\n",
       "0                0             0   \n",
       "1                1             1   \n",
       "2                2             2   \n",
       "3                3             3   \n",
       "4                4             4   \n",
       "...            ...           ...   \n",
       "113566       30500         30500   \n",
       "113567       30501         30501   \n",
       "113568       30502         30502   \n",
       "113569       30503         30503   \n",
       "113570       30504         30504   \n",
       "\n",
       "                                                   Tweets  \\\n",
       "0       I wish quick recovery for talented @TOLOnews J...   \n",
       "1       These chinese people on campus are so consider...   \n",
       "2       China‚Äôs Leader, Under Fire, Says He Led Corona...   \n",
       "3       RT @DarrenEuronews: Top 5 countries üåç with hig...   \n",
       "4       RT @MileyCyrus: Spain, you united in solidarit...   \n",
       "...                                                   ...   \n",
       "113566  RT @YourAnonNews: Yesterday, there were 140,92...   \n",
       "113567  RT @iloharare: In the face of #covid19, we run...   \n",
       "113568  RT @SAfridiOfficial: I‚Äôve been feeling unwell ...   \n",
       "113569  It must be this Gvt which is worse than corona...   \n",
       "113570  BREAKING: Deputy leader of Iran's parliament s...   \n",
       "\n",
       "                                  Date      Country  Vader-sentiment value  \\\n",
       "0            2020-04-29 23:33:40+00:00  afghanistan                 0.9022   \n",
       "1                  2020-01-28 23:41:13  afghanistan                 0.5777   \n",
       "2                  2020-02-15 23:17:09  afghanistan                -0.6124   \n",
       "3       Sat Jun 13 07:50:54 +0000 2020  afghanistan                 0.2732   \n",
       "4       Sat Jun 13 09:52:29 +0000 2020  afghanistan                 0.6249   \n",
       "...                                ...          ...                    ...   \n",
       "113566  Sat Jun 13 11:24:47 +0000 2020     zimbabwe                 0.0000   \n",
       "113567  Sat Jun 13 14:12:11 +0000 2020     zimbabwe                -0.3274   \n",
       "113568  Sat Jun 13 09:52:26 +0000 2020     zimbabwe                -0.8020   \n",
       "113569             2020-03-02 23:49:25     zimbabwe                -0.4767   \n",
       "113570             2020-03-03 23:57:40     zimbabwe                -0.4939   \n",
       "\n",
       "       Vader-sentiment  \n",
       "0             positive  \n",
       "1             positive  \n",
       "2             negative  \n",
       "3             positive  \n",
       "4             positive  \n",
       "...                ...  \n",
       "113566         Neutral  \n",
       "113567        negative  \n",
       "113568        negative  \n",
       "113569        negative  \n",
       "113570        negative  \n",
       "\n",
       "[113571 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('vaderDataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweets']=df['Tweets'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [word for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "df['Text_cleaned'] = df['Tweets'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "      <th>Vader-sentiment value</th>\n",
       "      <th>Vader-sentiment</th>\n",
       "      <th>Text_cleaned</th>\n",
       "      <th>Text_stemmed</th>\n",
       "      <th>Text_lemmatized_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I wish quick recovery for talented @TOLOnews J...</td>\n",
       "      <td>2020-04-29 23:33:40+00:00</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>positive</td>\n",
       "      <td>[wish, quick, recovery, talented, tolonews, jo...</td>\n",
       "      <td>[wish, quick, recoveri, talent, tolonew, journ...</td>\n",
       "      <td>[wish, quick, recoveri, talent, tolonew, journ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These chinese people on campus are so consider...</td>\n",
       "      <td>2020-01-28 23:41:13</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.5777</td>\n",
       "      <td>positive</td>\n",
       "      <td>[chinese, people, campus, considerate, wear, m...</td>\n",
       "      <td>[chines, peopl, campu, consider, wear, mask, o...</td>\n",
       "      <td>[chine, peopl, campu, consider, wear, mask, ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China‚Äôs Leader, Under Fire, Says He Led Corona...</td>\n",
       "      <td>2020-02-15 23:17:09</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>-0.6124</td>\n",
       "      <td>negative</td>\n",
       "      <td>[china, leader, fire, says, led, coronavirus, ...</td>\n",
       "      <td>[china, leader, fire, say, led, coronaviru, fi...</td>\n",
       "      <td>[china, leader, fire, say, led, coronaviru, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @DarrenEuronews: Top 5 countries üåç with hig...</td>\n",
       "      <td>Sat Jun 13 07:50:54 +0000 2020</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>positive</td>\n",
       "      <td>[rt, darreneuronews, top, countries, highest, ...</td>\n",
       "      <td>[rt, darreneuronew, top, countri, highest, num...</td>\n",
       "      <td>[rt, darreneuronew, top, countri, highest, num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @MileyCyrus: Spain, you united in solidarit...</td>\n",
       "      <td>Sat Jun 13 09:52:29 +0000 2020</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>positive</td>\n",
       "      <td>[rt, mileycyrus, spain, united, solidarity, bl...</td>\n",
       "      <td>[rt, mileycyru, spain, unit, solidar, black, l...</td>\n",
       "      <td>[rt, mileycyru, spain, unit, solidar, black, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  \\\n",
       "0  I wish quick recovery for talented @TOLOnews J...   \n",
       "1  These chinese people on campus are so consider...   \n",
       "2  China‚Äôs Leader, Under Fire, Says He Led Corona...   \n",
       "3  RT @DarrenEuronews: Top 5 countries üåç with hig...   \n",
       "4  RT @MileyCyrus: Spain, you united in solidarit...   \n",
       "\n",
       "                             Date      Country  Vader-sentiment value  \\\n",
       "0       2020-04-29 23:33:40+00:00  afghanistan                 0.9022   \n",
       "1             2020-01-28 23:41:13  afghanistan                 0.5777   \n",
       "2             2020-02-15 23:17:09  afghanistan                -0.6124   \n",
       "3  Sat Jun 13 07:50:54 +0000 2020  afghanistan                 0.2732   \n",
       "4  Sat Jun 13 09:52:29 +0000 2020  afghanistan                 0.6249   \n",
       "\n",
       "  Vader-sentiment                                       Text_cleaned  \\\n",
       "0        positive  [wish, quick, recovery, talented, tolonews, jo...   \n",
       "1        positive  [chinese, people, campus, considerate, wear, m...   \n",
       "2        negative  [china, leader, fire, says, led, coronavirus, ...   \n",
       "3        positive  [rt, darreneuronews, top, countries, highest, ...   \n",
       "4        positive  [rt, mileycyrus, spain, united, solidarity, bl...   \n",
       "\n",
       "                                        Text_stemmed  \\\n",
       "0  [wish, quick, recoveri, talent, tolonew, journ...   \n",
       "1  [chines, peopl, campu, consider, wear, mask, o...   \n",
       "2  [china, leader, fire, say, led, coronaviru, fi...   \n",
       "3  [rt, darreneuronew, top, countri, highest, num...   \n",
       "4  [rt, mileycyru, spain, unit, solidar, black, l...   \n",
       "\n",
       "                             Text_lemmatized_stemmed  \n",
       "0  [wish, quick, recoveri, talent, tolonew, journ...  \n",
       "1  [chine, peopl, campu, consider, wear, mask, ot...  \n",
       "2  [china, leader, fire, say, led, coronaviru, fi...  \n",
       "3  [rt, darreneuronew, top, countri, highest, num...  \n",
       "4  [rt, mileycyru, spain, unit, solidar, black, l...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df['Text_stemmed'] = df['Text_cleaned'].apply(lambda x: stemming(x))\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "df['Text_lemmatized_stemmed'] = df['Text_stemmed'].apply(lambda x: lemmatizer(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "le = LabelEncoder() \n",
    "#df['deceptive']= le.fit_transform(df['deceptive'])\n",
    "df['polarity']= le.fit_transform(df['Vader-sentiment'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104134    [death, incorrectli, label, coronaviru, death,...\n",
       "102373    [coronaviru, crisi, made, china, strategist, h...\n",
       "35694         [coronaviru, love, race, truli, ethnic, viru]\n",
       "13965                     [stay, safe, everyon, coronaviru]\n",
       "75874     [rememb, prematur, open, countri, back, week, ...\n",
       "                                ...                        \n",
       "56877     [dchd, identifi, possibl, coronaviru, exposur,...\n",
       "26123     [read, itll, explain, bet, human, havent, clue...\n",
       "42965     [much, toilet, paper, coronaviru, toilet, pape...\n",
       "99447     [democraci, leadership, polit, govern, deadlie...\n",
       "37588     [bill, protect, privat, renter, coronaviru, ou...\n",
       "Name: Text_lemmatized_stemmed, Length: 90856, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(df['polarity']).values\n",
    "X = df['Text_lemmatized_stemmed']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kirthika\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "TOP_K = 30000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val = X_train.values, X_test.values\n",
    "x_train, x_val, word_index, max_length = sequence_vectorize(x_train, x_val)\n",
    "num_features = min(len(word_index) + 1, TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90856 samples, validate on 22715 samples\n",
      "Epoch 1/20\n",
      "90856/90856 [==============================] - 73s 808us/sample - loss: 0.6506 - acc: 0.7333 - val_loss: 0.4335 - val_acc: 0.8384\n",
      "Epoch 2/20\n",
      "90856/90856 [==============================] - 75s 824us/sample - loss: 0.4244 - acc: 0.8481 - val_loss: 0.4111 - val_acc: 0.8465\n",
      "Epoch 3/20\n",
      "90856/90856 [==============================] - 79s 865us/sample - loss: 0.3630 - acc: 0.8704 - val_loss: 0.4170 - val_acc: 0.8480\n",
      "Epoch 4/20\n",
      "90856/90856 [==============================] - 76s 840us/sample - loss: 0.3250 - acc: 0.8849 - val_loss: 0.4267 - val_acc: 0.8506\n",
      "Epoch 5/20\n",
      "90856/90856 [==============================] - 76s 842us/sample - loss: 0.2961 - acc: 0.8950 - val_loss: 0.4408 - val_acc: 0.8472\n",
      "Epoch 6/20\n",
      "90856/90856 [==============================] - 85s 939us/sample - loss: 0.2769 - acc: 0.9026 - val_loss: 0.4555 - val_acc: 0.8458\n",
      "Epoch 7/20\n",
      "90856/90856 [==============================] - 75s 825us/sample - loss: 0.2596 - acc: 0.9094 - val_loss: 0.4730 - val_acc: 0.8460\n",
      "Epoch 8/20\n",
      "90856/90856 [==============================] - 73s 803us/sample - loss: 0.2451 - acc: 0.9144 - val_loss: 0.4996 - val_acc: 0.8458\n",
      "Epoch 9/20\n",
      "90856/90856 [==============================] - 1645s 18ms/sample - loss: 0.2320 - acc: 0.9191 - val_loss: 0.5208 - val_acc: 0.8429\n",
      "Epoch 10/20\n",
      "90856/90856 [==============================] - 74s 812us/sample - loss: 0.2233 - acc: 0.9221 - val_loss: 0.5178 - val_acc: 0.8423\n",
      "Epoch 11/20\n",
      "90856/90856 [==============================] - 74s 811us/sample - loss: 0.2144 - acc: 0.9254 - val_loss: 0.5314 - val_acc: 0.8399\n",
      "Epoch 12/20\n",
      "90856/90856 [==============================] - 74s 815us/sample - loss: 0.2086 - acc: 0.9275 - val_loss: 0.5538 - val_acc: 0.8370\n",
      "Epoch 13/20\n",
      "90856/90856 [==============================] - 264s 3ms/sample - loss: 0.1994 - acc: 0.9318 - val_loss: 0.5788 - val_acc: 0.8395\n",
      "Epoch 14/20\n",
      "90856/90856 [==============================] - 74s 810us/sample - loss: 0.1934 - acc: 0.9336 - val_loss: 0.5743 - val_acc: 0.8389\n",
      "Epoch 15/20\n",
      "90856/90856 [==============================] - 74s 816us/sample - loss: 0.1891 - acc: 0.9349 - val_loss: 0.5871 - val_acc: 0.8363\n",
      "Epoch 16/20\n",
      "90856/90856 [==============================] - 203s 2ms/sample - loss: 0.1858 - acc: 0.9361 - val_loss: 0.5874 - val_acc: 0.8346\n",
      "Epoch 17/20\n",
      "90856/90856 [==============================] - 77s 846us/sample - loss: 0.1809 - acc: 0.9376 - val_loss: 0.6286 - val_acc: 0.8383\n",
      "Epoch 18/20\n",
      "90856/90856 [==============================] - 75s 831us/sample - loss: 0.1763 - acc: 0.9391 - val_loss: 0.6230 - val_acc: 0.8382\n",
      "Epoch 19/20\n",
      "90856/90856 [==============================] - 75s 829us/sample - loss: 0.1711 - acc: 0.9420 - val_loss: 0.6270 - val_acc: 0.8347\n",
      "Epoch 20/20\n",
      "90856/90856 [==============================] - 84s 928us/sample - loss: 0.1704 - acc: 0.9422 - val_loss: 0.6238 - val_acc: 0.8357\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "embedding_vector_length = 256\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(TOP_K, embedding_vector_length, input_length=max_length),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist1 = model.fit(x_train, y_train, validation_data=(x_val, y_test), epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22715/22715 [==============================] - 2s 108us/sample - loss: 0.6238 - acc: 0.8357\n",
      "Test set\n",
      "  Loss: 0.624\n",
      "  Accuracy: 0.836\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(x_val,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
